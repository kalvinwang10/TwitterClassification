Multi class Classification: Predicting Twitter Users Motivation:We built a multiclass classifier to output a twitter user given a random tweet. We trained our algorithm on tweets from 4 user accounts – Bernie Sanders, Hillary Clinton, Barack Obama and Donald Trump. The election was characterized by large amounts of misinformation and fake news, with quotes being attributed to candidates who never said those words. Our classifier therefore attempts to associate popular political figures with their “correct” words.Problem Definition:To extract features from tweets to create a multiclass classifier - predicting the most likely user to have tweeted a particular tweet. Differences among tweets are classified by semantics. Challenges: Improving the Model & Algorithm: How do we define differences other than by semantics? To what extent should we consider retweets (different writing styles) in our feature extraction  Scraping Data: The Twitter API’s required 5 min rest period between calls made acquiring thousands of tweets time consuming.  Approach: Feature Extraction: We experimented with combinations of unigrams and bigrams, and found that having just bigrams minimizes both train and test errors.Loss Functions:We tried hinge loss, logistic loss and squared loss. The first two gave us comparable and small errors that indicated not much overfitting, while the last gave us large errorsHyper-parameters:  We automatically ran SGD with different N, eta and lambda (penalty), using a validation set to confirm our choices.Data Acquisition: To acquire the data needed to create our tweet classifier, we scraped over 12,000 tweets for Barrack Obama, Bernie Sanders, Donald Trump and Hillary Clinton – from the most recent tweet to tweets from 2013.  We removed each user’s twitter handle and tweet signature i.e. Bernie’s ‘–B’  and Hillary’s ‘-Hillary’ to remove any labeling of the text   Analysis Obama wasn’t participating in the election, so was often tweeting about very different topics. His tweets were least misclassified.We found bigrams with the highest weights, indicating candidate uniqueness: e.g. “political revolution”, “#DemsInPhilly”, “children and”“#DrainTheSwamp ”, “#ActOnClimate ”Adding unigrams increased our error because of overlapping topics in tweets.The confusion matrix were a result of the user’s using the same words and tweeting about the same topics. Our program did not check for intentions or meaning Future: Use Stanford NLP parser – gives a tree with “less important” words deeper in the tree. Run a DFS to weigh deeper words less than shallower words.Use Word2Vec – train to reconstruct linguistic contexts of words in tweets so words that share common contexts in data are located in close proximity (as vector)